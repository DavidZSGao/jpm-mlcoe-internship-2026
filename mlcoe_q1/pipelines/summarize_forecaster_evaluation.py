"""Summarise forecaster evaluation outputs, including MC interval diagnostics."""

from __future__ import annotations

import argparse
import logging
from pathlib import Path
from typing import Iterable, Mapping, Sequence

import numpy as np
import pandas as pd


def parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--evaluation",
        type=Path,
        default=Path(__file__).resolve().parents[2]
        / "reports/q1/artifacts/forecaster_evaluation.parquet",
        help="Path to the detailed evaluation parquet generated by evaluate_forecaster",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path(__file__).resolve().parents[2]
        / "reports/q1/artifacts/forecaster_evaluation_summary.parquet",
        help="Destination path for the aggregated summary parquet",
    )
    parser.add_argument(
        "--group-by",
        type=str,
        default="ticker,mode",
        help="Comma separated columns to group by when computing aggregates",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        help="Python logging level to use for status messages",
    )
    return parser.parse_args(argv)


def _parse_group_columns(raw: str) -> list[str]:
    if not raw.strip():
        return []
    return [token.strip() for token in raw.split(",") if token.strip()]


def _quantile_columns(columns: Iterable[str], prefix: str) -> Mapping[float, str]:
    quantiles: dict[float, str] = {}
    for column in columns:
        if not column.startswith(prefix):
            continue
        token = column[len(prefix) :]
        if not token:
            continue
        try:
            quantile = float(token) / 100.0
        except ValueError:
            continue
        quantiles[quantile] = column
    return dict(sorted(quantiles.items()))


def _coverage_rate(
    frame: pd.DataFrame,
    target_col: str,
    lower_col: str,
    upper_col: str,
) -> float:
    if frame.empty:
        return float("nan")
    mask = frame[[target_col, lower_col, upper_col]].notna().all(axis=1)
    if not mask.any():
        return float("nan")
    sliced = frame.loc[mask]
    covered = (sliced[target_col] >= sliced[lower_col]) & (
        sliced[target_col] <= sliced[upper_col]
    )
    return float(np.mean(covered.to_numpy(dtype=float)))


def _validate_required_columns(df: pd.DataFrame) -> None:
    required = {"assets_mae", "equity_mae", "identity_gap"}
    missing = required.difference(df.columns)
    if missing:
        raise KeyError(
            "Evaluation data is missing required columns: " + ", ".join(sorted(missing))
        )


def summarize(df: pd.DataFrame, group_cols: Sequence[str]) -> pd.DataFrame:
    _validate_required_columns(df)
    group_cols = list(group_cols)

    quantile_map_assets = _quantile_columns(df.columns, "pred_total_assets_q")
    quantile_map_equity = _quantile_columns(df.columns, "pred_equity_q")
    quantile_map_income = _quantile_columns(df.columns, "pred_net_income_q")

    df_local = df.copy()

    interval_specs = []
    if quantile_map_assets:
        quantiles = list(quantile_map_assets)
        lower_col = quantile_map_assets[quantiles[0]]
        upper_col = quantile_map_assets[quantiles[-1]]
        df_local["_assets_interval_width"] = df_local[upper_col] - df_local[lower_col]
        interval_specs.append(
            (
                "assets",
                "true_total_assets",
                "_assets_interval_width",
                lower_col,
                upper_col,
            )
        )
    if quantile_map_equity:
        quantiles = list(quantile_map_equity)
        lower_col = quantile_map_equity[quantiles[0]]
        upper_col = quantile_map_equity[quantiles[-1]]
        df_local["_equity_interval_width"] = df_local[upper_col] - df_local[lower_col]
        interval_specs.append(
            (
                "equity",
                "true_equity",
                "_equity_interval_width",
                lower_col,
                upper_col,
            )
        )
    if quantile_map_income:
        quantiles = list(quantile_map_income)
        lower_col = quantile_map_income[quantiles[0]]
        upper_col = quantile_map_income[quantiles[-1]]
        df_local["_income_interval_width"] = df_local[upper_col] - df_local[lower_col]
        interval_specs.append(
            (
                "net_income",
                "true_net_income",
                "_income_interval_width",
                lower_col,
                upper_col,
            )
        )

    aggregations: dict[str, tuple[str, str]] = {
        "observations": ("assets_mae", "count"),
        "assets_mae_mean": ("assets_mae", "mean"),
        "assets_mae_median": ("assets_mae", "median"),
        "assets_mae_max": ("assets_mae", "max"),
        "equity_mae_mean": ("equity_mae", "mean"),
        "equity_mae_median": ("equity_mae", "median"),
        "equity_mae_max": ("equity_mae", "max"),
        "identity_gap_mean": ("identity_gap", "mean"),
    }
    if "net_income_mae" in df_local.columns:
        aggregations.update(
            {
                "net_income_mae_mean": ("net_income_mae", "mean"),
                "net_income_mae_median": ("net_income_mae", "median"),
                "net_income_mae_max": ("net_income_mae", "max"),
            }
        )

    for prefix, _, width_col, _, _ in interval_specs:
        aggregations[f"{prefix}_interval_width_mean"] = (width_col, "mean")

    if group_cols:
        grouped = df_local.groupby(group_cols, dropna=False)
        summary = grouped.agg(**aggregations).sort_index()
    else:
        summary_data = {
            name: [df_local[col].agg(func)]
            for name, (col, func) in aggregations.items()
        }
        summary = pd.DataFrame(summary_data)

    for prefix, target_col, _, lower_col, upper_col in interval_specs:
        coverage_col = f"{prefix}_interval_coverage"
        if group_cols:
            assert "grouped" in locals()
            coverage = grouped.apply(
                lambda g: _coverage_rate(g, target_col, lower_col, upper_col),
                include_groups=False,
            )
            summary[coverage_col] = coverage.astype(float)
        else:
            summary[coverage_col] = float(
                _coverage_rate(df_local, target_col, lower_col, upper_col)
            )

    if group_cols:
        return summary.reset_index()
    return summary.reset_index(drop=True)


def summarize_metrics(df: pd.DataFrame, group_cols: Sequence[str]) -> pd.DataFrame:
    """Backward-compatible alias for ``summarize`` used by downstream modules."""

    return summarize(df, group_cols)


def main(argv: Sequence[str] | None = None) -> None:
    args = parse_args(argv)
    logging.basicConfig(level=getattr(logging, args.log_level.upper(), logging.INFO))

    df = pd.read_parquet(args.evaluation)
    if df.empty:
        raise ValueError("Evaluation parquet is empty; run evaluate_forecaster first")

    group_cols = _parse_group_columns(args.group_by)
    summary = summarize(df, group_cols)

    args.output.parent.mkdir(parents=True, exist_ok=True)
    summary.to_parquet(args.output, index=False)
    logging.info("Summary saved to %s", args.output)


if __name__ == "__main__":
    main()
